<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Ecosystem</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.9422d7ed.css" as="style"><link rel="preload" href="/assets/js/app.0774983a.js" as="script"><link rel="preload" href="/assets/js/2.f1c961ca.js" as="script"><link rel="preload" href="/assets/js/19.7fb5808b.js" as="script"><link rel="prefetch" href="/assets/js/10.e46d72ad.js"><link rel="prefetch" href="/assets/js/11.8254d096.js"><link rel="prefetch" href="/assets/js/12.ee9e402b.js"><link rel="prefetch" href="/assets/js/13.83e5ba5e.js"><link rel="prefetch" href="/assets/js/14.b28cb6fa.js"><link rel="prefetch" href="/assets/js/15.bc06572c.js"><link rel="prefetch" href="/assets/js/16.adf7ee00.js"><link rel="prefetch" href="/assets/js/17.638c8a0e.js"><link rel="prefetch" href="/assets/js/18.661a591c.js"><link rel="prefetch" href="/assets/js/20.ecf034ed.js"><link rel="prefetch" href="/assets/js/21.9b198a89.js"><link rel="prefetch" href="/assets/js/22.6a51d93b.js"><link rel="prefetch" href="/assets/js/23.0a1cfe35.js"><link rel="prefetch" href="/assets/js/24.b90ca5c4.js"><link rel="prefetch" href="/assets/js/25.33941b02.js"><link rel="prefetch" href="/assets/js/26.95b36b7b.js"><link rel="prefetch" href="/assets/js/27.3535f59c.js"><link rel="prefetch" href="/assets/js/28.64562af7.js"><link rel="prefetch" href="/assets/js/29.849624e7.js"><link rel="prefetch" href="/assets/js/3.7ef52252.js"><link rel="prefetch" href="/assets/js/30.cbe296a5.js"><link rel="prefetch" href="/assets/js/31.6cbf0b0f.js"><link rel="prefetch" href="/assets/js/32.5b861b2b.js"><link rel="prefetch" href="/assets/js/33.acf6a958.js"><link rel="prefetch" href="/assets/js/34.a957457a.js"><link rel="prefetch" href="/assets/js/35.1035d599.js"><link rel="prefetch" href="/assets/js/36.fcc7fbc8.js"><link rel="prefetch" href="/assets/js/37.bbcb34fe.js"><link rel="prefetch" href="/assets/js/38.326e2ab1.js"><link rel="prefetch" href="/assets/js/39.4db4134f.js"><link rel="prefetch" href="/assets/js/4.4c2548f6.js"><link rel="prefetch" href="/assets/js/40.b6150ce1.js"><link rel="prefetch" href="/assets/js/5.8a2b5103.js"><link rel="prefetch" href="/assets/js/6.3a5b4460.js"><link rel="prefetch" href="/assets/js/7.60024c79.js"><link rel="prefetch" href="/assets/js/8.bcfb9593.js"><link rel="prefetch" href="/assets/js/9.e0370369.js">
    <link rel="stylesheet" href="/assets/css/0.styles.9422d7ed.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="ecosystem"><a href="#ecosystem" class="header-anchor">#</a> Ecosystem</h1> <h2 id="platform-init"><a href="#platform-init" class="header-anchor">#</a> platform-init</h2> <p>This Command Line Interactive (CLI) utility creates the structure for a platform's parser. It asks a series of questions and generates the repository structure for the parser with a manifest.json file, a parser's skeleton and an empty test file. The command is interactive and doesn't take any parameter.</p> <p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token builtin class-name">cd</span> ezpaarse/
<span class="token builtin class-name">.</span> ./bin/env
platform-init
</code></pre></div><h2 id="pkb-cleaner"><a href="#pkb-cleaner" class="header-anchor">#</a> pkb-cleaner</h2> <p>Detects and deletes duplicates in the knowledge bases.</p> <div class="language-bash extra-class"><pre class="language-bash"><code>Usage: pkb-cleaner <span class="token punctuation">[</span>-nvp<span class="token punctuation">]</span> <span class="token punctuation">[</span>DIR_TO_CLEAN<span class="token punctuation">]</span>

Options:
  --platform, -p   Name of a platform whose PKB should be cleaned.<span class="token punctuation">(</span>if provided, ignore <span class="token function">dir</span> path<span class="token punctuation">)</span>
  --norewrite, -n  If provided, <span class="token keyword">do</span> not rewrite files once the check is complete.
  --verbose, -v    Print all duplicated entries
</code></pre></div><p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code>pkb-cleaner ./path/to/some/directory
pkb-cleaner --platform<span class="token operator">=</span>sd
</code></pre></div><h2 id="scrape"><a href="#scrape" class="header-anchor">#</a> scrape</h2> <p>Launches the scrapers for one or more platforms. The scrapers are little utility programs to assemble a knowledge base by scraping a publisher's website.</p> <div class="language-bash extra-class"><pre class="language-bash"><code>Usage: /home/yan/ezpaarse/bin/scrape <span class="token punctuation">[</span>-alvfc<span class="token punctuation">]</span> <span class="token punctuation">[</span>Platform<span class="token punctuation">]</span> <span class="token punctuation">[</span>Platform<span class="token punctuation">]</span> <span class="token punctuation">..</span>.

Options:
  --all, -a      Execute all scrapers.
  --list, -l     Only list scrapers without executing them.
  --clean, -c    Clean PKB files when all scrapers has been executed.
  --force, -f    Overwrite PKB files <span class="token keyword">if</span> they already exist.
  --verbose, -v  Print scrapers output into the console.
</code></pre></div><p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code>scrape sd cbo <span class="token comment"># launches the scrapers for SD (ScienceDirect) and CBO</span>
scrape -al    <span class="token comment"># lists all the existing scrapers without launching them</span>
</code></pre></div><h2 id="loginjector"><a href="#loginjector" class="header-anchor">#</a> loginjector</h2> <p>Streams a log file to a local instance of ezPAARSE.</p> <p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code>zcat monezproxy.log.gz <span class="token operator">|</span> ./bin/loginjector
</code></pre></div><p>Usage:</p> <div class="language- extra-class"><pre class="language-text"><code>Injects data into ezPAARSE and gets the response
Usage: node ./loginjector

Options:
  --input, -i     a file to inject into ezPAARSE (default: stdin)
  --output, -o    a file to send the result to (default: stdout)
  --server, -s    the server to send the request to (ex: http://ezpaarse.com:80). If none, will send to a local instance.
  --proxy, -p     the proxy which generated the log file
  --format, -f    the format of log lines (ex: %h %u [%t] &quot;%r&quot;)
  --encoding, -e  encoding of sent data (gzip, deflate)
  --accept, -a    wanted type for the response (text/csv, application/json)
</code></pre></div><p>This command eases the sending of log files to an ezPAARSE instance, compared to the cURL utility.</p> <h2 id="loganonymizer"><a href="#loganonymizer" class="header-anchor">#</a> loganonymizer</h2> <p>Anonymizes a log file. The sensitive elements, like the login, machine name or IP address, are replaced with random values. The log file should be sent to the system input (stdin) of the command.</p> <p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code>zcat monezproxy.log.gz <span class="token operator">|</span> ./bin/loganonymizer
</code></pre></div><p>Usage:</p> <div class="language- extra-class"><pre class="language-text"><code>Anonymize critical data in a log file
Usage: node ./loganonymizer --input=[string] --output=[string] --proxy=[string] --format[string]

Options:
  --input, -i   the input data to clean
  --output, -o  the destination where to send the result to
  --proxy, -p   the proxy which generated the log file
  --format, -f  the format of log lines (ex: %h %u [%t] &quot;%r&quot;)
</code></pre></div><p>This is useful for generating test files by removing sensitive items (related to the protection of personal data). Each value is replaced by the same random value so keeping associations and be able to deduplicate is guaranteed.</p> <h2 id="logextractor"><a href="#logextractor" class="header-anchor">#</a> logextractor</h2> <p>Retrieves one or more fields in a log file. The log file should be sent to the system input (stdin) of the command.</p> <p>Examples:</p> <div class="language-bash extra-class"><pre class="language-bash"><code>zcat monezproxy.log.gz <span class="token operator">|</span> ./bin/logextractor --fields<span class="token operator">=</span>url
zcat monezproxy.log.gz <span class="token operator">|</span> ./bin/logextractor --fields<span class="token operator">=</span>login,url --separator<span class="token operator">=</span><span class="token string">&quot;|&quot;</span>
</code></pre></div><p>Usage:</p> <div class="language- extra-class"><pre class="language-text"><code>Extract specific fields from a log stream
Usage: node ./logextractor --fields=[string] --separator=&quot;;&quot;

Options:
  --fields, -f            fields to extract from log lines (ex: url,login,host)  [required]
  --separator, --sep, -s  character to use between each field                    [required]  [default: &quot;\t&quot;]
  --input, -i             a file to extract the fields from (default: stdin)
  --output, -o            a file to write the result into (default: stdout)
  --proxy, -p             the proxy which generated the log file
  --format, -t            the format of log lines (ex: %h %u [%t] &quot;%r&quot;)

</code></pre></div><p>This is useful for manipulating log files. A common use is extracting URLs from a log file in order to analyze a platform for a publisher. For example, here's how to get the URL for the ScienceDirect platform by sorting alphabetically and deduplicating them:</p> <div class="language-bash extra-class"><pre class="language-bash"><code>zcat monezproxy.log.gz <span class="token operator">|</span> ./bin/logextractor --field<span class="token operator">=</span>url <span class="token operator">|</span> <span class="token function">grep</span> <span class="token string">&quot;sciencedirect&quot;</span> <span class="token operator">|</span> <span class="token function">sort</span> <span class="token operator">|</span> <span class="token function">uniq</span>
</code></pre></div><h2 id="csvextractor"><a href="#csvextractor" class="header-anchor">#</a> csvextractor</h2> <p>Extracts content from a CSV file. The CSV file must be sent to the system input (stdin) of the command.</p> <p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token function">cat</span> monfichier.csv <span class="token operator">|</span> ./bin/csvextractor
</code></pre></div><p>Usage:</p> <div class="language- extra-class"><pre class="language-text"><code>Parse a csv source into json.
  Usage: csvextractor [-sc] [-f string | -d string | -k string] [--no-header]

Options:
  --file, -f          A csv file to parse. If absent, will read from standard input.
  --fields, -d        A list of fields to extract. Default extract all fields. (Ex: --fields issn,pid)
  --key, -k           If provided, the matching field will be used as a key in the resulting json.
  --silent, -s        If provided, empty values or unexisting fields won't be showed in the results.
  --csv, -c           If provided, the result will be a csv.
  --json, -j          If provided, the result will be a JSON.
  --jsonstream, --js  If provided, the result will be a JSON stream (one JSON per line).
  --noheader          If provided, the result won't have a header line. (if csv output)
</code></pre></div><p>This command is useful for testing the parser directly from the test file by extracting the URL column of the file.</p> <p>Example (parser test):</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token function">cat</span> ./test/npg.2013-01-16.csv <span class="token operator">|</span> <span class="token punctuation">..</span>/<span class="token punctuation">..</span>/bin/csvextractor --fields<span class="token operator">=</span><span class="token string">'url'</span> -c --noheader <span class="token operator">|</span> ./parser.js
</code></pre></div><h2 id="csvtotalizer"><a href="#csvtotalizer" class="header-anchor">#</a> csvtotalizer</h2> <p>Produces a summary on the content of a CSV file resulting from a processing of ezPAARSE. The CSV file must be sent to the system input (stdin) of the command.</p> <p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token function">cat</span> monresultat.csv <span class="token operator">|</span> ./bin/csvtotalizer
</code></pre></div><p>Usage:</p> <div class="language- extra-class"><pre class="language-text"><code>Summarize fields from a CSV stream
Usage: node ./bin/csvtotalizer --fields=[string] --output=&quot;text|json&quot;

Options:
  --output, -o  output : text or json                                        [required]  [default: &quot;text&quot;]
  --sort, -s    sort : asc or desc in text mode                              [required]  [default: &quot;desc&quot;]
  --fields, -f  fields to compute from the CSV (ex: domain;host;login;type)  [required]  [default: &quot;domain;host;login;type&quot;]
</code></pre></div><p>This is useful for getting a quick overview of a processing outcome of a log file ezPAARSE.
By default, domain fields, host, login and type are available in text format.
Here is how to know how many different consultation events have been recognized in a sample file:</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token function">cat</span> ./test/dataset/sd.2012-11-30.300.log <span class="token operator">|</span> ./bin/loginjector <span class="token operator">|</span> ./bin/csvtotalizer
</code></pre></div><h2 id="logfaker"><a href="#logfaker" class="header-anchor">#</a> logfaker</h2> <p>Generates an output stream matching with log lines of a platform on stdout.</p> <p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code>./logfaker <span class="token operator">|</span> ./loginjector
</code></pre></div><p>Usage:</p> <div class="language- extra-class"><pre class="language-text"><code>Usage: node ./logfaker --platform=[string] --nb=[num] --rate=[num] --duration=[num]

Options:
  --platform      the publisher platform code used as a source for generating url  [required]  [default: &quot;sd&quot;]
  --nb, -n        number of lines of log to generate                               [required]  [default: &quot;nolimit&quot;]
  --rate, -r      number of lines of log to generate per second (max 1000)         [required]  [default: 10]
  --duration, -d  stop log generation after a specific number of seconds           [required]  [default: &quot;nolimit&quot;]
</code></pre></div><p>Useful to test the performance of ezPAARSE.</p> <h2 id="pkbvalidator"><a href="#pkbvalidator" class="header-anchor">#</a> pkbvalidator</h2> <p>Checks the validity of a knowledge base for a publisher's platform.
This file must conform to the KBART format.</p> <p>This command checks the following:</p> <ul><li>The presence of the .txt extension</li> <li>Uniqueness of title_id</li> <li>Minimal identification information available</li> <li>Syntax check of standardized identifiers (ISSN, ISBN, DOI)</li></ul> <p>Usage:</p> <div class="language- extra-class"><pre class="language-text"><code>Check a platform knowledge base file.
  Usage: node ./bin/pkbvalidator [-cfsv] pkb_file1.txt [pkb_file2.txt]

Options:
  --silent, -s   If provided, no output generated.
  --csv, -c      If provided, the error-output will be a csv.
  --verbose, -v  show stats of checking.
</code></pre></div><h2 id="ezp-process"><a href="#ezp-process" class="header-anchor">#</a> ezp process</h2> <p>Let you process one or more files with an instance of ezPAARSE. If no files are provided, the command will listen to <code>stdin</code>. The results are printed to <code>stdout</code>, unless you set an output file with <code>--out</code>.</p> <div class="language-bash extra-class"><pre class="language-bash"><code>Options:
  --output, --out, -o       Output <span class="token function">file</span>
  --header, --headers, -H   Add a header to the request <span class="token punctuation">(</span>ex: <span class="token string">&quot;<span class="token variable"><span class="token variable">`</span>Reject-Files: all<span class="token variable">`</span></span>&quot;</span><span class="token punctuation">)</span>
  --download, -d            Download a <span class="token function">file</span> from the job directory
  --verbose, -v             Shows detailed operations.
  --settings, -s            Set a predefined setting.
</code></pre></div><p>Examples of use :</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># Simple case, process ezproxy.log and write results to result.csv</span>
ezp process ezproxy.log --out result.csv

<span class="token comment"># Same as above, and download the report file</span>
ezp process ezproxy.log --out result.csv --download job-report.html

<span class="token comment"># Download the report file with a custom path</span>
ezp process ezproxy.log --out result.csv --download job-report.html:./reports/report.html

<span class="token comment"># Reading from stdin and redirecting stdout to file</span>
<span class="token function">cat</span> ezproxy.log <span class="token operator">|</span> ezp process <span class="token operator">&gt;</span> result.csv
</code></pre></div><h2 id="ezp-bulk"><a href="#ezp-bulk" class="header-anchor">#</a> ezp bulk</h2> <p>Process files in <code>sourceDir</code> and save results in <code>destDir</code>. If <code>destDir</code> is not provided, results will be stored in <code>sourceDir</code>, aside the source files. When processing files recursively with the <code>-r</code> option, <code>destDir</code> will mimic the structure of <code>sourceDir</code>. Files will use the same or Files with existing results are skipped, unless the <code>--force</code> flag is set. By default, the result file and the job report are downloaded, but you can get additionnal files from the job directory by using the <code>--download</code> option.</p> <div class="language-bash extra-class"><pre class="language-bash"><code>Options:
  --header, --headers, -H   Add a header to the request <span class="token punctuation">(</span>ex: <span class="token string">&quot;<span class="token variable"><span class="token variable">`</span>Reject-Files: all<span class="token variable">`</span></span>&quot;</span><span class="token punctuation">)</span>
  --settings, -s            Set a predefined setting.
  --recursive, -r           Look <span class="token keyword">for</span> log files into subdirectories
  --download, -d            Download a <span class="token function">file</span> from the job directory
  --overwrite, --force, -f  Overwrite existing files
  --verbose, -v             Shows detailed operations.
  --list, -l                Only list log files <span class="token keyword">in</span> the directory
</code></pre></div><p>Examples of use :</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># Simple case, processing files recursively from ezproxy-logs and storing results in ezproxy-results</span>
ezp bulk -r ezproxy-logs/ ezproxy-results/

<span class="token comment"># Activating reject files and downloading unqualified log lines along results</span>
ezp bulk -r ezproxy-logs/ ezproxy-results/ -H <span class="token string">&quot;Reject-Files: all&quot;</span> --download lines-unqualified-ecs.log
</code></pre></div><p>A result file (<code>.ec.csv</code> extension) and a report in HTML format (extension<code>.report.html</code>) are generated in the output directory for each log file. If the destination directory is not specified, they are generated in the same directory as the file being processed.
If an error occurs when processing a file, the incomplete result file is named with the <code>.ko</code> extension.
Rejects files are not retained by ezPAARSE.</p> <div class="language- extra-class"><pre class="language-text"><code>Inject files to ezPAARSE (for batch purpose)
  Usage: /home/yan/ezpaarse/bin/ecbulkmaker [-rflvH] SOURCE_DIR [RESULT_DIR]

Options:
  --recursive, -r  If provided, files in subdirectories will be processed. (preserves the file tree)
  --list, -l       If provided, only list files.
  --force, -f      override existing result (default false).
  --header, -H     header parameter to use.
  --verbose, -v    Shows detailed operations.

</code></pre></div><h3 id="video-demonstration"><a href="#video-demonstration" class="header-anchor">#</a> Video Demonstration</h3> <p>This <a href="https://www.youtube.com/watch?v=5Tlk6GECSTI" target="_blank" rel="noopener noreferrer">screencast<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> demonstrates the usage of ecbulkmaker (ie process a directory containing log files and outputting a mirror directory with the results)</p> <h2 id="hostlocalize"><a href="#hostlocalize" class="header-anchor">#</a> hostlocalize</h2> <p>Enriches a csv result file containing a host name with the geolocation of the IP address</p> <p>Example:</p> <div class="language-bash extra-class"><pre class="language-bash"><code>./hostlocalize -f ezpaarsedata.csv <span class="token operator">&gt;</span> ezpaarsedatalocalised.csv
</code></pre></div><p>The input file is assumed to contain a field with the ip address for the location</p> <div class="language- extra-class"><pre class="language-text"><code>Enrich a csv with geolocalisation from host ip.
  Usage: node ./bin/hostlocalize [-s] [-f string | -k string]

Options:
  --hostkey, -k  the field name containing host ip (default &quot;host&quot;).
  --file, -f     A csv file to parse. If absent, will read from standard input.
</code></pre></div></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.0774983a.js" defer></script><script src="/assets/js/2.f1c961ca.js" defer></script><script src="/assets/js/19.7fb5808b.js" defer></script>
  </body>
</html>
