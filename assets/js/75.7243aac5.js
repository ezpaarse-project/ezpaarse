(window.webpackJsonp=window.webpackJsonp||[]).push([[75],{397:function(e,a,t){"use strict";t.r(a);var s=t(10),r=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"ecosystem"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ecosystem"}},[e._v("#")]),e._v(" Ecosystem")]),e._v(" "),a("h2",{attrs:{id:"pkb-cleaner"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pkb-cleaner"}},[e._v("#")]),e._v(" pkb-cleaner")]),e._v(" "),a("p",[e._v("Detects and deletes duplicates in the knowledge bases.")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("Usage: pkb-cleaner "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v("-nvp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v("DIR_TO_CLEAN"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v("\n\nOptions:\n  --platform, "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-p")]),e._v("   Name of a platform whose PKB should be cleaned."),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("if provided, ignore "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("dir")]),e._v(" path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n  --norewrite, "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-n")]),e._v("  If provided, "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("do")]),e._v(" not rewrite files once the check is complete.\n  --verbose, "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-v")]),e._v("    Print all duplicated entries\n")])])]),a("p",[e._v("Example:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("pkb-cleaner ./path/to/some/directory\npkb-cleaner "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--platform")]),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v("sd\n")])])]),a("h2",{attrs:{id:"scrape"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scrape"}},[e._v("#")]),e._v(" scrape")]),e._v(" "),a("p",[e._v("Launches the scrapers for one or more platforms. The scrapers are little utility programs to assemble a knowledge base by scraping a publisher's website.")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("Usage: /home/yan/ezpaarse/bin/scrape "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v("-alvfc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v("Platform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("[")]),e._v("Platform"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("]")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("..")]),e._v(".\n\nOptions:\n  --all, "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-a")]),e._v("      Execute all scrapers.\n  --list, "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-l")]),e._v("     Only list scrapers without executing them.\n  --clean, "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-c")]),e._v("    Clean PKB files when all scrapers has been executed.\n  --force, "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-f")]),e._v("    Overwrite PKB files "),a("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("if")]),e._v(" they already exist.\n  --verbose, "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-v")]),e._v("  Print scrapers output into the console.\n")])])]),a("p",[e._v("Example:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("scrape sd cbo "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# launches the scrapers for SD (ScienceDirect) and CBO")]),e._v("\nscrape "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-al")]),e._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# lists all the existing scrapers without launching them")]),e._v("\n")])])]),a("h2",{attrs:{id:"loginjector"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#loginjector"}},[e._v("#")]),e._v(" loginjector")]),e._v(" "),a("p",[e._v("Streams a log file to a local instance of ezPAARSE.")]),e._v(" "),a("p",[e._v("Example:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("zcat monezproxy.log.gz "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/loginjector\n")])])]),a("p",[e._v("Usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('Injects data into ezPAARSE and gets the response\nUsage: node ./loginjector\n\nOptions:\n  --input, -i     a file to inject into ezPAARSE (default: stdin)\n  --output, -o    a file to send the result to (default: stdout)\n  --server, -s    the server to send the request to (ex: http://ezpaarse.com:80). If none, will send to a local instance.\n  --proxy, -p     the proxy which generated the log file\n  --format, -f    the format of log lines (ex: %h %u [%t] "%r")\n  --encoding, -e  encoding of sent data (gzip, deflate)\n  --accept, -a    wanted type for the response (text/csv, application/json)\n')])])]),a("p",[e._v("This command eases the sending of log files to an ezPAARSE instance, compared to the cURL utility.")]),e._v(" "),a("h2",{attrs:{id:"loganonymizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#loganonymizer"}},[e._v("#")]),e._v(" loganonymizer")]),e._v(" "),a("p",[e._v("Anonymizes a log file. The sensitive elements, like the login, machine name or IP address, are replaced with random values. The log file should be sent to the system input (stdin) of the command.")]),e._v(" "),a("p",[e._v("Example:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("zcat monezproxy.log.gz "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/loganonymizer\n")])])]),a("p",[e._v("Usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('Anonymize critical data in a log file\nUsage: node ./loganonymizer --input=[string] --output=[string] --proxy=[string] --format[string]\n\nOptions:\n  --input, -i   the input data to clean\n  --output, -o  the destination where to send the result to\n  --proxy, -p   the proxy which generated the log file\n  --format, -f  the format of log lines (ex: %h %u [%t] "%r")\n')])])]),a("p",[e._v("This is useful for generating test files by removing sensitive items (related to the protection of personal data). Each value is replaced by the same random value so keeping associations and be able to deduplicate is guaranteed.")]),e._v(" "),a("h2",{attrs:{id:"logextractor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#logextractor"}},[e._v("#")]),e._v(" logextractor")]),e._v(" "),a("p",[e._v("Retrieves one or more fields in a log file. The log file should be sent to the system input (stdin) of the command.")]),e._v(" "),a("p",[e._v("Examples:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("zcat monezproxy.log.gz "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/logextractor "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--fields")]),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v("url\nzcat monezproxy.log.gz "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/logextractor "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--fields")]),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v("login,url "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--separator")]),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[e._v('"|"')]),e._v("\n")])])]),a("p",[e._v("Usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('Extract specific fields from a log stream\nUsage: node ./logextractor --fields=[string] --separator=";"\n\nOptions:\n  --fields, -f            fields to extract from log lines (ex: url,login,host)  [required]\n  --separator, --sep, -s  character to use between each field                    [required]  [default: "\\t"]\n  --input, -i             a file to extract the fields from (default: stdin)\n  --output, -o            a file to write the result into (default: stdout)\n  --proxy, -p             the proxy which generated the log file\n  --format, -t            the format of log lines (ex: %h %u [%t] "%r")\n\n')])])]),a("p",[e._v("This is useful for manipulating log files. A common use is extracting URLs from a log file in order to analyze a platform for a publisher. For example, here's how to get the URL for the ScienceDirect platform by sorting alphabetically and deduplicating them:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("zcat monezproxy.log.gz "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/logextractor "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--field")]),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v("url "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("grep")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[e._v('"sciencedirect"')]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("sort")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("uniq")]),e._v("\n")])])]),a("h2",{attrs:{id:"csvextractor"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#csvextractor"}},[e._v("#")]),e._v(" csvextractor")]),e._v(" "),a("p",[e._v("Extracts content from a CSV file. The CSV file must be sent to the system input (stdin) of the command.")]),e._v(" "),a("p",[e._v("Example:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[e._v("cat")]),e._v(" monfichier.csv "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/csvextractor\n")])])]),a("p",[e._v("Usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Parse a csv source into json.\n  Usage: csvextractor [-sc] [-f string | -d string | -k string] [--no-header]\n\nOptions:\n  --file, -f          A csv file to parse. If absent, will read from standard input.\n  --fields, -d        A list of fields to extract. Default extract all fields. (Ex: --fields issn,pid)\n  --key, -k           If provided, the matching field will be used as a key in the resulting json.\n  --silent, -s        If provided, empty values or unexisting fields won't be showed in the results.\n  --csv, -c           If provided, the result will be a csv.\n  --json, -j          If provided, the result will be a JSON.\n  --jsonstream, --js  If provided, the result will be a JSON stream (one JSON per line).\n  --noheader          If provided, the result won't have a header line. (if csv output)\n")])])]),a("p",[e._v("This command is useful for testing the parser directly from the test file by extracting the URL column of the file.")]),e._v(" "),a("p",[e._v("Example (parser test):")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[e._v("cat")]),e._v(" ./test/npg.2013-01-16.csv "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("..")]),e._v("/"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("..")]),e._v("/bin/csvextractor "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--fields")]),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[e._v("'url'")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-c")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--noheader")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./parser.js\n")])])]),a("h2",{attrs:{id:"csvtotalizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#csvtotalizer"}},[e._v("#")]),e._v(" csvtotalizer")]),e._v(" "),a("p",[e._v("Produces a summary on the content of a CSV file resulting from a processing of ezPAARSE. The CSV file must be sent to the system input (stdin) of the command.")]),e._v(" "),a("p",[e._v("Example:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[e._v("cat")]),e._v(" monresultat.csv "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/csvtotalizer\n")])])]),a("p",[e._v("Usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('Summarize fields from a CSV stream\nUsage: node ./bin/csvtotalizer --fields=[string] --output="text|json"\n\nOptions:\n  --output, -o  output : text or json                                        [required]  [default: "text"]\n  --sort, -s    sort : asc or desc in text mode                              [required]  [default: "desc"]\n  --fields, -f  fields to compute from the CSV (ex: domain;host;login;type)  [required]  [default: "domain;host;login;type"]\n')])])]),a("p",[e._v("This is useful for getting a quick overview of a processing outcome of a log file ezPAARSE.\nBy default, domain fields, host, login and type are available in text format.\nHere is how to know how many different consultation events have been recognized in a sample file:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[e._v("cat")]),e._v(" ./test/dataset/sd.2012-11-30.300.log "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/loginjector "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./bin/csvtotalizer\n")])])]),a("h2",{attrs:{id:"logfaker"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#logfaker"}},[e._v("#")]),e._v(" logfaker")]),e._v(" "),a("p",[e._v("Generates an output stream matching with log lines of a platform on stdout.")]),e._v(" "),a("p",[e._v("Example:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("./logfaker "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" ./loginjector\n")])])]),a("p",[e._v("Usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('Usage: node ./logfaker --platform=[string] --nb=[num] --rate=[num] --duration=[num]\n\nOptions:\n  --platform      the publisher platform code used as a source for generating url  [required]  [default: "sd"]\n  --nb, -n        number of lines of log to generate                               [required]  [default: "nolimit"]\n  --rate, -r      number of lines of log to generate per second (max 1000)         [required]  [default: 10]\n  --duration, -d  stop log generation after a specific number of seconds           [required]  [default: "nolimit"]\n')])])]),a("p",[e._v("Useful to test the performance of ezPAARSE.")]),e._v(" "),a("h2",{attrs:{id:"pkbvalidator"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pkbvalidator"}},[e._v("#")]),e._v(" pkbvalidator")]),e._v(" "),a("p",[e._v("Checks the validity of a knowledge base for a publisher's platform.\nThis file must conform to the KBART format.")]),e._v(" "),a("p",[e._v("This command checks the following:")]),e._v(" "),a("ul",[a("li",[e._v("The presence of the .txt extension")]),e._v(" "),a("li",[e._v("Uniqueness of title_id")]),e._v(" "),a("li",[e._v("Minimal identification information available")]),e._v(" "),a("li",[e._v("Syntax check of standardized identifiers (ISSN, ISBN, DOI)")])]),e._v(" "),a("p",[e._v("Usage:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Check a platform knowledge base file.\n  Usage: node ./bin/pkbvalidator [-cfsv] pkb_file1.txt [pkb_file2.txt]\n\nOptions:\n  --silent, -s   If provided, no output generated.\n  --csv, -c      If provided, the error-output will be a csv.\n  --verbose, -v  show stats of checking.\n")])])]),a("h2",{attrs:{id:"hostlocalize"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hostlocalize"}},[e._v("#")]),e._v(" hostlocalize")]),e._v(" "),a("p",[e._v("Enriches a csv result file containing a host name with the geolocation of the IP address")]),e._v(" "),a("p",[e._v("Example:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[e._v("./hostlocalize "),a("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("-f")]),e._v(" ezpaarsedata.csv "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v(">")]),e._v(" ezpaarsedatalocalised.csv\n")])])]),a("p",[e._v("The input file is assumed to contain a field with the ip address for the location")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('Enrich a csv with geolocalisation from host ip.\n  Usage: node ./bin/hostlocalize [-s] [-f string | -k string]\n\nOptions:\n  --hostkey, -k  the field name containing host ip (default "host").\n  --file, -f     A csv file to parse. If absent, will read from standard input.\n')])])])])}),[],!1,null,null,null);a.default=r.exports}}]);